% Emacs, this is -*-latex-*-

\title{\href{https://github.com/vicente-gonzalez-ruiz/scalar_quantization}{Scalar (Digital) Quantization}}

\author{Vicente Gonz√°lez Ruiz}

\maketitle
\tableofcontents

\section{Definition}
%{{{

\begin{figure}
  \svgfig{graphics/Q}{3cm}{300pt}
  \caption{Scalar quantization and dequantization of a signal.}
  \label{fig:Q}
\end{figure}

Scalar (Digital) Quantization (SQ) (see Figure~\ref{fig:Q}) is a
technique in which each source sample is quantized independently from
the other samples and therefore, a quantization index ${\mathbf k}_i$
is produced for each input sample ${\mathbf
  s}_i$~\cite{vruiz__quantization}.

A $K$-levels SQ $Q$ performs a partition of the domain of ${\mathbf
  s}$ into $K$ cells ${\mathbf C}_k, k = 1, \cdots, K$ and associates
to any ${\mathbf s}_i$ the quantization index $k$ if ${\mathbf s}_i\in
{\mathbf C}_k$.

The smallest and the highest value of all ${\mathbf C}_k$ are called
the decision boundaries of $Q$. Therefore,
\begin{equation}
  Q({\mathbf s}_i) = {\mathbf k}_i \Leftrightarrow {\mathbf C}_{k-1} <
  {\mathbf s}_i \le {\mathbf C}_k.
\end{equation}

The inverse quantizer $Q^{-1}$ estimates ${\mathbf s}_i$ knowing
${\mathbf k}_i$ and possibly the PDF $p_{\mathbf S}({\mathbf s})$,
using a reconstruction level ${\mathbf r}_k\in ]{\mathbf C}_{k-1},
  {\mathbf C}_k]$, generating the output
\begin{equation}
  \tilde{\mathbf s}_i = {\mathbf r}_k.
\end{equation}

%}}}

\section{Uniform SQ (USQ)}
%{{{

In an USQ, all decission levels are equally spaced by a distance known
as \emph{the quantization step size} $\Delta$, satisfiying that the
domain of the input signal is divided into intervals of constant size
\begin{equation}
  \Delta=d_{i+1}-d_i=r_{i+1}-r_i,
\end{equation}
where $d_i$ is the $i$-th decission level and $r_i$ is the $i$-th
representation level.
  %Therefore,
  %\begin{equation}
  %  Q=\frac{1}{\Delta}
  %\end{equation}
  %and
%We define the number of decision levels as
%\begin{equation}
%  Q=\frac{d_{\text max}-d_{\text min}}{\Delta}.
%  \label{eq:delta_definition}
%\end{equation}

In USQs, the quantization error ${\mathbf e}$ depends on $\Delta$ and
can be modeled as a noise signal that: (1) is uncorrelated to the
input ${\mathbf s}$, (2) is
\href{https://en.wikipedia.org/wiki/White_noise}{white} and therefore,
(3) it follows a uniform distribution.

%However, the quantization
%step
%$\Delta<<\href{https://en.wikipedia.org/wiki/Standard_deviation}{\sigma_x}$~\cite{vetterli1995wavelets}.

%Under the premise that $e$ is uniform, and considering that
%$y_i=(x_{i-1}+x_i)/2$ (something quite reasonable when $x$ can also be
%considered as uniform) the average quantization error is $\frac{Z}{4}$
%($\frac{Z}{2}$ is the meximum and $0$ is the minimum), and for this
%particular case
%\begin{equation}
%  \text{MSE} =
%  \frac{1}{\Delta}\int_{-\Delta/2}^{\Delta/2}e^2de=\frac{\Delta^2}{12}.
%  \label{eq:MSE_uniform_scalar_quantizer}
%\end{equation}

%}}}

\section{Mid-rise USQ}
%{{{

In mid-rise quantizers the reconstructed signal $\tilde{\mathbf s}$
never is 0, even if ${\mathbf s}_i=0$ for any $i$.

The mapping process in a mid-rise quantizer can be described as
\begin{equation}
  {\mathbf k}_i = \lfloor \frac{{\mathbf s}_i}{\Delta} \rfloor,
  \label{eq:mid-rise}
\end{equation}
and the inverse mapping by
\begin{equation}
  \tilde{\mathbf s}_i = \Delta({\mathbf k}_i + \frac{1}{2}).
  \label{eq:inverse_mid-rise}
\end{equation}

\begin{figure}
  \svgfig{graphics/midrise}{6cm}{600pt}
  \caption{An uniform mid-rise quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/midrise.ipynb}{notebook}). $\Delta=1$
    and $Q=13$ (the decision boundaries have been ignored). The
    decision levels ($x$) are $\{\cdots,-3,-2,-1,0,1,2,3,\cdots\}$
    and the representation levels ($y$) are
    $\{\cdots,-2.5,-1.5,-0.5,0.5,1.5,2.5,\cdots\}$.}
  \label{fig:midrise}
\end{figure}
%}}}

\section{Mid-tread USQ}
%{{{ 

In mid-tread quantizers the reconstructed signal is 0 when ${\mathbf
  s}_i=0$.

The mapping process in a mid-tread quantizer can be described as
\begin{equation}
  {\mathbf k}_i = \text{round}( \frac{{\mathbf s}_i}{\Delta} ),
  \label{eq:mid-rise}
\end{equation}
and the inverse mapping by
\begin{equation}
  \tilde{\mathbf s}_i = \Delta{\mathbf k}_i.
  \label{eq:inverse_mid-rise}
\end{equation}

\begin{figure}
  \svg{graphics/midtread}{6cm}{600pt}
  \caption{An uniform mid-tread quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/midtread.ipynb}{notebook}). $\Delta=1$
    and $Q=12$ (the decision boundaries have been ignored). The
    decision levels ($x$) are $\{\cdots,-2.5,-1.5,-0.5,0.5,1.5,2.5,\cdots\}$
    and the representation levels ($y$) are
    $\{\cdots,-2,-1,-0,1,2,\cdots\}$.}
  \label{fig:midtread}
\end{figure}
%}}}

\section{Mid-tread USQ with deadzone}
%{{{

The quantization step is $2\Delta$ for ${\mathbf s}_i=0$. Deadzone
quantizers tends to remove the
\href{https://en.wikipedia.org/wiki/Noise_(electronics)}{electronic
  noise} (that usually has a small amplitude compared to the input
signal ${\mathbf s}$), precisely where the signal-to-noise ratio is
lowest.\footnote{Notice that, by definition, dead-zone quantizers
should not be considered uniform, and that all dead-zone quantizers,
by definition, are mid-tread.}

\begin{figure}
  \svg{graphics/deadzone}{6cm}{600pt}
  \caption{An uniform deadzone quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/deadzone.ipynb}{notebook}). $\Delta=1$
    and $Q=12$ (the decision boundaries have been ignored). The
    decision levels ($x$) are $\{\cdots,-3,-2,-1,1,2,3,\cdots\}$
    and the representation levels ($y$) are
    $\{\cdots,-2,-1,-0,1,2,\cdots\}$.}
  \label{fig:deadzone}
\end{figure}
%}}}

\section{Non-uniform quantization}
%{{{

If we known that the input signal ${\mathbf s}$ does not follow an
uniform distribution, it is possible to use a variable $\Delta$ to
minimize the quantization error ${\mathbf e}$ in the most part of the
samples.

%}}}

\section{Companded quantization~\cite{sayood2017introduction}}
%{{{

\href{https://en.wikipedia.org/wiki/Companding}{Companding}
(COMpressing + exPANDING) quantization is used when most of the
samples are concentrated arround 0, or as happens with humans, most of
the (interesting) audio has a low volume (for this reason, companded
quantizers are used in telephony). It is, therefore, and non-uniform
quantizer.

The original signal is mapped through a compressor, quantized using an
uniform quantized, and re-mapped using the corresponding expander. The
result is a logarithmic quantization.
\href{https://en.wikipedia.org/wiki/\%CE\%9C-law_algorithm}{\(\mu\)-law}
example:
\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/companded_quantization.ipynb}{notebook}

\begin{figure}
  \svg{graphics/ulaw-compressor}{600}
  \svg{graphics/ulaw-expander}{600}
  \svg{graphics/companded}{600}
  \caption{Insights of a companded quantizer.}
  \label{fig:companded_quantizer}
\end{figure}

%}}}

\section{PDF-optimized quantization}
%{{{

This non-uniform quantizer is a generalization of the companded
quantizer, where the samples can follow any distribution. Now, with
the idea of minimizing the distortion (in general, in terms of the
MSE), we chose $\Delta_i$ smaller where signal samples appear most
offen.

The most used PDF (Probability Density Function) quantizer is the
Max-LLoyd quantizer, who developed an iterative algorithm for
determining the decision and representation levels.

\svg{graphics/cuantif_max-lloyd}{600}

%}}}

\section{Adaptive quantization}
%{{{

Implies to modify $\Delta$ dynamically, depending on the local
characteristics of ${\mathbf s}$.

%}}}

\section{Forward adaptive quantization}
%{{{

\begin{itemize}
\item
  Used for determining a suitable \(\Delta\) for blocks of samples.
\item ~
  \hypertarget{encoder}{%
  \subsubsection*{Encoder:}\label{encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While samples in \(s\):

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read into \(b\) the next \(B\) samples of \(s\).
    \item
      Determine \(\Delta\), minimizing the quantization error, and
      output \(\Delta\) (or the data necessary for its determination).
    \item
      Quantize \(b\) and output it.
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection*{Decoder:}\label{decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While data in input:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read \(\Delta\) (or the data necessary for determining it, and in
      this case, use the same algorithm that the used by the encoder).
    \item
      ``Dequantize'' \(b\) and output it (note that the dequantization
      is only a way of calling the process of reverting the original
      range of the quantized signal).
    \end{enumerate}
  \end{enumerate}
\item
  The selection of \(B\) is a trade-off between the increase in side
  information needed by small block sizes and the loss of fidelity due
  to large block sizes.
\item
  Forward adaptive quantization generates a
  \(B\text{-samples}\times f_s\) delay (buffering), where \(f_s\) is the
  sampling rate of \(s\).
\end{itemize}

%}}}

\section{Backward adaptive quantization}
%{{{

\begin{itemize}
\item
  Only the previously quantized samples are available to use in
  adapting the quantizer.
\item
  Idea: If happens that \(\Delta\) is smaller than it should be, the
  input will fall in the outer levels of the quantizer a high number
  of times. On the other hand, if \(\Delta\) is larger than it should
  be, the samples will fall in the inner levels a high number of
  times.
\item ~
  \hypertarget{encoder}{%
  \subsubsection*{Encoder:}\label{encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(s\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Quantize the next sample.
    \item
      Observe the output and refine \(\Delta\).
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection*{Decoder:}\label{decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(\hat{s}\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      ``Dequantize'' the next sample.
    \item
      Step 2.B of the encoder.
    \end{enumerate}
  \end{enumerate}
\end{itemize}

%}}}

\section{The Jayant quantizer~\cite{jayant1974digital}}
%{{{

\begin{itemize}
\item
  Adaptive quantization with a one word memory (\(\Delta_{(t-1)}\)).
\item
  A Jayant quantider defines the Step 2.B. as: Define a multiplier
  \(M_l\) for each quantization level \(l\), where for the inner levels
  \(M_l<1\) and for the outer levels \(M_l>1\), and compute:

  \[
    \Delta^{[n]} = \Delta^{[n-1]}{M_l}^{[n-1]},
  \]

  where \(\Delta^{[n-1]}\) was the previous quantization step and
  \({M_l}^{[n-1]}\) the level multiplier for the \(n-1\)-th (previous)
  sample. Thus, if the previous (\(n-1\)) quantization used a
  \(\Delta^{[n-1]}\) too small (using outer quantization levels) then
  \(\Delta^{[n]}\) will be larger and viceversa.
\item
  Depending on the multipliers \(M\), the quantizer will converge or
  oscillate. In the first case, the quantizer will be good for small
  variations of \(s\) but bad when a fast adaption to large changes in
  \(s\) is required. In the second one, the quantizer will adapt quickly
  to fast variations of \(s\) but will oscillate when \(s\) changles
  slowly.
\item
  Most Jayant quantizers clip the computation of \(\Delta\) to avoid
  generating a zero output quantizer in those contexts where \(s\) is
  zero or very close to zero, and to improve the adaptation to smaller
  samples after a sequence of bigger ones (avoiding to grow without
  limit):

  \[
  \begin{array}{ll}
    \text{if}~\Delta^{[n]}<\Delta_{\text{min}}~\text{then}~\Delta^{[n]} = \Delta_{\text{min}},\\
    \text{if}~\Delta^{[n]}>\Delta_{\text{max}}~\text{then}~\Delta^{[n]} = \Delta_{\text{max}}.
  \end{array}
  \]
\end{itemize}

%}}}

\section{Adapting with a scale factor}
%{{{

\begin{itemize}
\item
  A Jayant quantized adapts the quantization step to the dynamic range
  of the signa using a set of multipiers. A similar effect can be
  provided by dividing the input signal by a scale factor defined
  iteratively as:

  \begin{equation}
    \alpha^{[n]} = \alpha^{[n-1]}M_l^{[n-1]}.
  \end{equation}
\end{itemize}

%}}}

\section{Perceptual quantization}
%{{{

An important consideration is the relative perfectual importance of
the input samples. This leads to a weighting of the MSE at the
output. The weighting function can be derived through experiments to
determine the ``level of just noticeable noise''. For example, in
subband coding, as expected, high frequecy subbands tolerate more
noise because the HAS (Human Auditory System) becomes less sensitive
at them.

%}}}


\section{Resources}
\bibliography{quantization,DWT,data-compression}
