% Emacs, this is -*-latex-*-

\title{\href{https://github.com/vicente-gonzalez-ruiz/quantization}{Quantization}}

\author{Vicente Gonz√°lez Ruiz}

\maketitle

This discussion is reffered only to the
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization}
of digital signals. To see how quantization is applied to analog
signal, see
\href{https://vicente-gonzalez-ruiz.github.io/analog_quantization/}{this}.

\section{Basics}
%{{{ 

A (digital)
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{\emph{quantizer}},
usually denoted by $Q$, is an encoding system that inputs a (digital)
signal $\mathbf{s}$ (a sequence of digital signal samples) and outputs
a sequence of quantization indexes ${\mathbf k}$. The inverse system,
called a \emph{dequantizer} (denoted by $\text{Q}^{-1}$), generally
only recovers an approximated version of ${\mathbf s}$ that it will be
denoted by $\tilde{{\mathbf s}}$.

If we define the \emph{cardinality} operator $|\cdot|$ applied to a
signal as the number of different values that such signal can take,
i.e., the size of the signal alphabet, it ususally holds that
\begin{equation}
  |{\mathbf s}|\leq|\tilde{\mathbf s}| = |{\mathbf k}|.
\end{equation}
As a consequence of this property, the values that ${\mathbf k}$, individually,
can take will require less bits to be represented than the values that
the original signal ${\mathbf s}$ can. This is the key concept used in
the \href{https://en.wikipedia.org/wiki/Lossy_compression}{lossy
  compression} of signals.

%}}}

\section{Scalar Quantization (SQ)}
%{{{

\begin{figure}
  \centering \myfig{graphics/Q}{3cm}{300}
  \caption{Scalar quantization and dequantization of a signal.}
  \label{fig:Q}
\end{figure}

A SQ produce a quantization index ${\mathbf k}_i$ for each input sample
${\mathbf s}_i$ (see Fig.~\ref{fig:Q}). Here, we define the quantization error
\begin{equation}
  {\mathbf e}_i = {\mathbf s}_i - \tilde{{\mathbf s}}_i.
\end{equation}
Notice that, in general, this signal should be minimized in order to
reduce the distortion generated by the quantization process.

%}}}

\section{Uniform SQ}
%{{{

In an USQ (Uniform SQ), all decission levels are equally spaced by a
distance known as \emph{the quantization step size} (or simply,
\emph{quantization step}) \(\Delta\), satisfiying that the input range
is divided into intervals of constant size
\begin{equation}
  \Delta=d_{i+1}-d_i=r_{i+1}-r_i,
\end{equation}
where $d_i$ is the $i$-th decissio level and $r_i$ is the $i$-th
representation level.
  %Therefore,
  %\begin{equation}
  %  Q=\frac{1}{\Delta}
  %\end{equation}
  %and
%We define the number of decision levels as
%\begin{equation}
%  Q=\frac{d_{\text max}-d_{\text min}}{\Delta}.
%  \label{eq:delta_definition}
%\end{equation}

In a USQ, the quantization error ${\mathbf e}$ depends on the so
called quantization step $\Delta$ and can be modeled as a noise signal
that: (1) is uncorrelated to the input ${\mathbf s}$, (2) is
\href{https://en.wikipedia.org/wiki/White_noise}{white} and therefore,
(3) it follows a uniform distribution.

%However, the quantization
%step
%$\Delta<<\href{https://en.wikipedia.org/wiki/Standard_deviation}{\sigma_x}$~\cite{vetterli1995wavelets}.

%Under the premise that $e$ is uniform, and considering that
%$y_i=(x_{i-1}+x_i)/2$ (something quite reasonable when $x$ can also be
%considered as uniform) the average quantization error is $\frac{Z}{4}$
%($\frac{Z}{2}$ is the meximum and $0$ is the minimum), and for this
%particular case
%\begin{equation}
%  \text{MSE} =
%  \frac{1}{\Delta}\int_{-\Delta/2}^{\Delta/2}e^2de=\frac{\Delta^2}{12}.
%  \label{eq:MSE_uniform_scalar_quantizer}
%\end{equation}

%}}}

\section{Midrise USQ}
%{{{ 
\begin{figure}
  \svg{graphics/midrise}{600}
  \caption{An uniform midrise quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/midrise.ipynb}{notebook}). $\Delta=1$
    and $Q=13$ (the decision boundaries have been ignored). The
    decision levels ($x$) are $\{\cdots,-3,-2,-1,0,1,2,3,\cdots\}$
    and the representation levels ($y$) are
    $\{\cdots,-2.5,-1.5,-0.5,0.5,1.5,2.5,\cdots\}$.}
  \label{fig:midrise}
\end{figure}
%}}}

\section{Midread USQ}
%{{{ 
\begin{figure}
  \svg{graphics/midtread}{600}
  \caption{An uniform midtread quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/midtread.ipynb}{notebook}). $\Delta=1$
    and $Q=12$ (the decision boundaries have been ignored). The
    decision levels ($x$) are $\{\cdots,-2.5,-1.5,-0.5,0.5,1.5,2.5,\cdots\}$
    and the representation levels ($y$) are
    $\{\cdots,-2,-1,-0,1,2,\cdots\}$.}
  \label{fig:midtread}
\end{figure}
%}}}

\section{Midread USQ with deadzone}
%{{{ 
\begin{figure}
  \svg{graphics/deadzone}{600}
  \caption{An uniform deadzone quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/deadzone.ipynb}{notebook}). $\Delta=1$
    and $Q=12$ (the decision boundaries have been ignored). The
    decision levels ($x$) are $\{\cdots,-3,-2,-1,1,2,3,\cdots\}$
    and the representation levels ($y$) are
    $\{\cdots,-2,-1,-0,1,2,\cdots\}$.}
  \label{fig:deadzone}
\end{figure}
%}}}

\section{Non-uniform quantization}
%{{{
If we known that the input signal ${\mathbf s}$ does not follow an
uniform distribution, it is possible to use a variable $\Delta$ to
minimize the quantization error ${\mathbf e}$ in the most part of the
samples.

%}}}

\section{Companded quantization~\cite{sayood2017introduction}}
%{{{

\href{https://en.wikipedia.org/wiki/Companding}{Companding}
(COMpressing + exPANDING) quantization is used when most of the
samples are concentrated arround 0. It is, therefore, and non-uniform
quantizer.

The original signal is mapped through a compressor, quantized using an
uniform quantized, and re-mapped using the corresponding expander. The
result is a logarithmic quantization.
\href{https://en.wikipedia.org/wiki/\%CE\%9C-law_algorithm}{\(\mu\)-law}
example:
\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/quantization/blob/master/graphics/companded_quantization.ipynb}{notebook}

\begin{figure}
  \svg{graphics/ulaw-compressor}{600}
  \svg{graphics/ulaw-expander}{600}
  \svg{graphics/companded}{600}
  \caption{Insights of a companded quantizer.}
  \label{fig:companded_quantizer}
\end{figure}

%}}}

\section{PDF-optimized quantization}
%{{{

This non-uniform quantizer is a generalization of the companded
quantizer, where the samples can follow any distribution. The most
used PDF (Probability Density Function) quantizer is the Max-LLoyd
quantizer:

\svg{graphics/cuantif_max-lloyd}{600}

%}}}

\section{Adaptive quantization}
%{{{

Implies to modify $\Delta$ dynamically, depending on the local characteristics of ${\mathbf s}$.

%}}}

\section{Forward adaptive quantization}
%{{{

\begin{itemize}
\item
  Used for determining a suitable \(\Delta\) for blocks of samples.
\item ~
  \hypertarget{encoder}{%
  \subsubsection*{Encoder:}\label{encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While samples in \(s\):

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read into \(b\) the next \(B\) samples of \(s\).
    \item
      Determine \(\Delta\), minimizing the quantization error, and
      output \(\Delta\) (or the data necessary for its determination).
    \item
      Quantize \(b\) and output it.
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection*{Decoder:}\label{decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While data in input:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read \(\Delta\) (or the data necessary for determining it, and in
      this case, use the same algorithm that the used by the encoder).
    \item
      ``Dequantize'' \(b\) and output it (note that the dequantization
      is only a way of calling the process of reverting the original
      range of the quantized signal).
    \end{enumerate}
  \end{enumerate}
\item
  The selection of \(B\) is a trade-off between the increase in side
  information needed by small block sizes and the loss of fidelity due
  to large block sizes.
\item
  Forward adaptive quantization generates a
  \(B\text{-samples}\times f_s\) delay (buffering), where \(f_s\) is the
  sampling rate of \(s\).
\end{itemize}

%}}}

\section{Backward adaptive quantization}
%{{{

\begin{itemize}
\item
  Only the previously quantized samples are available to use in
  adapting the quantizer.
\item
  Idea: If happens that \(\Delta\) is smaller than it should be, the
  input will fall in the outer levels of the quantizer a high number
  of times. On the other hand, if \(\Delta\) is larger than it should
  be, the samples will fall in the inner levels a high number of
  times.
\item ~
  \hypertarget{encoder}{%
  \subsubsection*{Encoder:}\label{encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(s\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Quantize the next sample.
    \item
      Observe the output and refine \(\Delta\).
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection*{Decoder:}\label{decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(\hat{s}\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      ``Dequantize'' the next sample.
    \item
      Step 2.B of the encoder.
    \end{enumerate}
  \end{enumerate}
\end{itemize}

%}}}

\section{The Jayant quantizer~\cite{jayant1974digital}}
%{{{

\begin{itemize}
\item
  Adaptive quantization with a one word memory (\(\Delta_{(t-1)}\)).
\item
  A Jayant quantider defines the Step 2.B. as: Define a multiplier
  \(M_l\) for each quantization level \(l\), where for the inner levels
  \(M_l<1\) and for the outer levels \(M_l>1\), and compute:

  \[
    \Delta^{[n]} = \Delta^{[n-1]}{M_l}^{[n-1]},
  \]

  where \(\Delta^{[n-1]}\) was the previous quantization step and
  \({M_l}^{[n-1]}\) the level multiplier for the \(n-1\)-th (previous)
  sample. Thus, if the previous (\(n-1\)) quantization used a
  \(\Delta^{[n-1]}\) too small (using outer quantization levels) then
  \(\Delta^{[n]}\) will be larger and viceversa.
\item
  Depending on the multipliers \(M\), the quantizer will converge or
  oscillate. In the first case, the quantizer will be good for small
  variations of \(s\) but bad when a fast adaption to large changes in
  \(s\) is required. In the second one, the quantizer will adapt quickly
  to fast variations of \(s\) but will oscillate when \(s\) changles
  slowly.
\item
  Most Jayant quantizers clip the computation of \(\Delta\) to avoid
  generating a zero output quantizer in those contexts where \(s\) is
  zero or very close to zero, and to improve the adaptation to smaller
  samples after a sequence of bigger ones (avoiding to grow without
  limit):

  \[
  \begin{array}{ll}
    \text{if}~\Delta^{[n]}<\Delta_{\text{min}}~\text{then}~\Delta^{[n]} = \Delta_{\text{min}},\\
    \text{if}~\Delta^{[n]}>\Delta_{\text{max}}~\text{then}~\Delta^{[n]} = \Delta_{\text{max}}.
  \end{array}
  \]
\end{itemize}

%}}}

\section{Adapting with a scale factor}
%{{{

\begin{itemize}
\item
  A Jayant quantized adapts the quantization step to the dynamic range
  of the signa using a set of multipiers. A similar effect can be
  provided by dividing the input signal by a scale factor defined
  iteratively as:

  \begin{equation}
    \alpha^{[n]} = \alpha^{[n-1]}M_l^{[n-1]}.
  \end{equation}
\end{itemize}

%}}}

\section{Vector quantization}
%{{{

in VQ, samples are quantized in groups (\emph{vectors}), producing a
quantization index by vector.  Usually, the lengths of the
quantization indexes are much shorter than the lengths of the vectors,
generating the compression. Vector Quantization (VQ) can remove
auto-correlation in the encoded signal and therefore, is more
efficient than Scalar Quantization (SQ). Unfortunately, the
computational requirements of VQ are, by far, much higher than the
needed by SQ. If we also consider that there are other techniques
(such as transform coding, that we will see later) that are able to
decorrelate the samples requiring less computational resources than
VQ, we can understand why SQ has been selected, for example, in most
\href{https://en.wikipedia.org/wiki/Image_compression}{image} and
\href{https://en.wikipedia.org/wiki/Video_coding_format}{video
  codecs}.

%}}}

\section{Perceptual quantization}
%{{{

An important consideration is the relative perfectual importante of
the input samples. This leads to a weighting of the MSE at the
output. The weighting function can be derived through experiments to
determine the ``level of just noticeable noise''. For example, in
subband coding, as expected, high freq. subbands tolerate more noise
because the HVS becomes less sensitive at them.

%}}}

\bibliography{quantization,DWT,data-compression}
