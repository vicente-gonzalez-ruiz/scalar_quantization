{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [(Scalar) Quantization](https://en.wikipedia.org/wiki/Quantization\\_\\(signal\\_processing\\)\n",
    "* Let:\n",
    "\n",
    "  \\begin{equation}\n",
    "    s_s(t)=s(nT_s),\n",
    "  \\end{equation}\n",
    "  \n",
    "  where $n\\in{\\mathbb{Z}}$ and $T_s$ is the sampling frequency in Hertzs.\n",
    "  \n",
    "* Quantizers discretize the amplitude of a [PAM signal](https://en.wikipedia.org/wiki/Pulse-amplitude_modulation) $s(nT_s)$, producing a [PCM signal](https://en.wikipedia.org/wiki/Pulse-code_modulation) and a loss of information.\n",
    "\n",
    "* The quantization process can be modeled as\n",
    "\n",
    "\\begin{equation}\n",
    "  s[n] = s(nT_s) + e(nT_s),\n",
    "\\end{equation}\n",
    "\n",
    "being $s[n]$ the the quantized signal and $e(nT_s)$ the quantization error.\n",
    "\n",
    "* Depending on the number of $Q$ different possible values (or *bins*) for $s[]$, we speak of a $q=\\lceil\\log_2(Q)\\rceil$-bits quantizer (this means that the output of the quantizer are $q$ bits for each sample, or that we have $2^q$ representation levels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform (lineal) quantization\n",
    "\n",
    "* All quantizers are defined from their set of $d_i; i\\in {\\mathbb{Z}}$ (decision levels) and $r_i; i\\in {\\mathbb{Z}}$ (representation levels). In a linear quantizer, the quantization step $\\Delta$ satisfies that\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\Delta=d_{i+1}-d_i=r_{i+1}-r_i.\n",
    "  \\end{equation}\n",
    "  \n",
    "  Notice that, for a given dynamic range of $s$, $Q$ is inversely proportional to $\\Delta$, and viceversa.\n",
    "  \n",
    "* In uniform quantizers, $\\Delta$ does not depends on the PAM sample values.\n",
    "\n",
    "  <img src=\"graphics/lineal_quantization.svg\" width=600>\n",
    "\n",
    "  Notice that in this quantizer, $e(nTs)_\\text{max}=\\frac{\\Delta}{2}$. This is a $q=\\lceil\\log_2(5)\\rceil=3$-bits quantizer ($Q=8$).\n",
    "\n",
    "* Uniform quantizers are used in most A/D (analogic/digital) converters, were it is expected the generation of uniformely distributed sequences of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (uniform quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r's(nT_s)~~\\lceil s(nT_s)/\\Delta\\rceil ~~s[n]=\\Delta\\times\\lceil s(nT_s)/\\Delta\\rceil~~e(nT_s)=s(nT_s)-s[n]'))\n",
    "for i in range(-16,16):\n",
    "    sample = i\n",
    "    quantized_sample = (i//2)*2+0.5\n",
    "    quantization_error = sample - quantized_sample\n",
    "    print(\"{:+4d}     {:+3d}           {:>+5}                    {:>+4}\".\\\n",
    "          format(sample, i//2, quantized_sample, quantization_error))\n",
    "display(Math(r'\\text{In this example}, \\Delta=2.0.~\\text{Therefore, for the defined input range}~[-16.0,15.0],\\\n",
    "        \\text{this is a}~q=4\\text{-bits quantizer}~(Q=16).'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization + Encoding\n",
    "\n",
    "* When we quantize digital signals, these are sequences of digital samples represented by symbols of a given alphabet, typically, a subset of ${\\mathbb{Z}}$ or ${\\mathbb{N}}$. Therefore, both the input and the output of the quantizer are indexes, not real values of a sampled signal.\n",
    "* Therefore, in this context, $\\Delta$ represents the length of the intervals of indexes what will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Quantize [Jfk_berlin_address_high.ogg](https://upload.wikimedia.org/wikipedia/commons/3/3a/Jfk_berlin_address_high.ogg) using $\\Delta=2$. Compute the variance of both audio sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-uniform quantization\n",
    "\n",
    "* In order to minimize the maximun, average or the total quantization error, $\\Delta$ can be adapted to the characteristics of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companded quantization\n",
    "\n",
    "* Non-uniform quantizer.\n",
    "\n",
    "* [Companding](https://en.wikipedia.org/wiki/Companding): COMpressing + exPANDING. The original signal is mapped through a compressor, quantized using an uniform quantized, and re-mapped using the corresponding expander. The result is a logarithmic quantization.\n",
    "\n",
    "* [$\\mu$-law](https://en.wikipedia.org/wiki/%CE%9C-law_algorithm) example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-6, 6, 500)\n",
    "y = np.round(x)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Uniform (midtread) quantizer')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_label_coords(1,0.35)\n",
    "ax.xaxis.set_label_text('Input')\n",
    "ax.yaxis.set_label_coords(0.45,.9)\n",
    "ax.yaxis.set_label_text('Output')\n",
    "ax.grid()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-6, 6, 500)\n",
    "y = np.floor(x)+0.5\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Uniform (midrise) quantizer')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_label_coords(1,0.35)\n",
    "ax.xaxis.set_label_text('Input')\n",
    "ax.yaxis.set_label_coords(0.45,.9)\n",
    "ax.yaxis.set_label_text('Output')\n",
    "ax.grid()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-6, 6, 500)\n",
    "y = x.astype(np.int)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Uniform (deadzone) quantizer')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_label_coords(1,0.35)\n",
    "ax.xaxis.set_label_text('Input')\n",
    "ax.yaxis.set_label_coords(0.45,.9)\n",
    "ax.yaxis.set_label_text('Output')\n",
    "ax.grid()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-6, 6, 500)\n",
    "mu = 1.0\n",
    "x_max = 1.0\n",
    "y = x_max*np.log(1+mu*np.abs(x)/x_max)/np.log(1+mu)*np.sign(x)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('$\\mu$-law compressor')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_label_coords(1,0.40)\n",
    "ax.xaxis.set_label_text('Input')\n",
    "ax.yaxis.set_label_coords(0.45,.9)\n",
    "ax.yaxis.set_label_text('Output')\n",
    "ax.grid()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-3, 3, 500)\n",
    "mu = 1.0\n",
    "x_max = 1.0\n",
    "y = (x_max/mu)*((1+mu)**(np.abs(x)/x_max)-1)*np.sign(x)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('$\\mu$-law expander')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_label_coords(1,0.40)\n",
    "ax.xaxis.set_label_text('Input')\n",
    "ax.yaxis.set_label_coords(0.45,.9)\n",
    "ax.yaxis.set_label_text('Output')\n",
    "ax.grid()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-6, 6, 500)\n",
    "mu = 1.0\n",
    "x_max = 1.0\n",
    "_y = np.floor(x_max*np.log(1+mu*np.abs(x)/x_max)/np.log(1+mu)*np.sign(x))+0.5\n",
    "y = (x_max/mu)*((1+mu)**(np.abs(_y)/x_max)-1)*np.sign(_y)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Companded quantizer')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_label_coords(1,0.40)\n",
    "ax.xaxis.set_label_text('Input')\n",
    "ax.yaxis.set_label_coords(0.45,.9)\n",
    "ax.yaxis.set_label_text('Output')\n",
    "ax.grid()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF-optimized quantization\n",
    "\n",
    "* Non-uniform quantizer.\n",
    "\n",
    "* if we known the probability distribution of the samples, we can select a small $\\Delta$ for the most probable samples and viceversa. \n",
    "\n",
    "<img src=\"data/cuantif_max-lloyd.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive quantization\n",
    "\n",
    "* Useful when the characteristics of $s$ (the variance, for example) vary over time.\n",
    "\n",
    "* Typically, the quantizer varies $\\Delta$ depending on such characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward adaptive quantization\n",
    "\n",
    "* Used for determining a suitable $\\Delta$ for blocks of samples.\n",
    "\n",
    "* ### Encoder:\n",
    "\n",
    "    1. While samples in $s$:\n",
    "        1. Read into $b$ the next $B$ samples of $s$.\n",
    "        2. Determine $\\Delta$, minimizing the quantization error, and output $\\Delta$ (or the data necessary for its determination).\n",
    "        3. Quantize $b$ and output it.\n",
    "\n",
    "* ### Decoder:\n",
    "\n",
    "    1. While data in input:\n",
    "        1. Read $\\Delta$ (or the data necessary for determining it, and in this case, use the same algorithm that the used by the encoder).\n",
    "        2. \"Dequantize\" $b$ and output it (note that the dequantization is only a way of calling the process of reverting the original range of the quantized signal).\n",
    "\n",
    "* The selection of $B$ is a trade-off between the increase in side information needed by small block sizes and the loss of fidelity due to large block sizes.\n",
    "\n",
    "* Forward adaptive quantization generates a $B\\text{-samples}\\times f_s$ delay (buffering), where $f_s$ is the sampling rate of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward adaptive quantization\n",
    "\n",
    "* Only the previously quantized samples are available to use in adapting the quantizer.\n",
    "\n",
    "* Idea: If happens that $\\Delta$ is smaller than it should be, the input will fall in the outer levels of the quantizer a high number of times. On the other hand, if $\\Delta$ is larger than it should be, the samples will fall in the inner levels a high number of times.\n",
    "\n",
    "* ### Encoder:\n",
    "\n",
    "    1. $\\Delta\\leftarrow 2$.\n",
    "    2. While $s$ is not exhausted:\n",
    "        1. Quantize the next sample.\n",
    "        2. Observe the output and refine $\\Delta$. \n",
    "        \n",
    "* ### Decoder:\n",
    "\n",
    "    1. $\\Delta\\leftarrow 2$.\n",
    "    2. While $\\hat{s}$ is not exhausted:\n",
    "        1. \"Dequantize\" the next sample.\n",
    "        2. Step 2.B of the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jayant quantizer  [[Jayant, 1974]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=%22Digital+coding+of+speech+waveforms%3A+PCM%2C+DPCM%2C+and+DM+quantizers%22&btnG=)\n",
    "\n",
    "* Adaptive quantization with a one word memory ($\\Delta_{(t-1)}$).\n",
    "\n",
    "* A Jayant quantider defines the Step 2.B. as: Define a multiplier $M_l$ for each quantization level $l$, where for the inner levels $M_l<1$ and for the outer levels $M_l>1$, and compute:\n",
    "\n",
    "  $$\n",
    "    \\Delta^{[n]} = \\Delta^{[n-1]}{M_l}^{[n-1]},\n",
    "  $$\n",
    "\n",
    "  where $\\Delta^{[n-1]}$ was the previous quantization step and ${M_l}^{[n-1]}$ the level multiplier for the $n-1$-th (previous) sample. Thus, if the previous ($n-1$) quantization used a $\\Delta^{[n-1]}$ too small (using outer quantization levels) then $\\Delta^{[n]}$ will be larger and viceversa.\n",
    "\n",
    "* Depending on the multipliers $M$, the quantizer will converge or oscillate. In the first case, the quantizer will be good for small variations of $s$ but bad when a fast adaption to large changes in $s$ is required. In the second one, the quantizer will adapt quickly to fast variations of $s$ but will oscillate when $s$ changles slowly.\n",
    "\n",
    "* Most Jayant quantizers clip the computation of $\\Delta$ to avoid generating a zero output quantizer in those contexts where $s$ is zero or very close to zero, and to improve the adaptation to smaller samples after a sequence of bigger ones (avoiding to grow without limit):\n",
    "\n",
    "  $$\n",
    "  \\begin{array}{ll}\n",
    "    \\text{if}~\\Delta^{[n]}<\\Delta_\\text{min}~\\text{then}~\\Delta^{[n]} = \\Delta_\\text{min},\\\\\n",
    "    \\text{if}~\\Delta^{[n]}>\\Delta_\\text{max}~\\text{then}~\\Delta^{[n]} = \\Delta_\\text{max}.\n",
    "  \\end{array}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting with a scale factor\n",
    "\n",
    "* A Jayant quantized adapts the quantization step to the dynamic range of the signa using a set of multipiers. A similar effect can be provided by dividing the input signal by a scale factor defined iteratively as:\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\alpha^{[n]} = \\alpha^{[n-1]}M_l^{[n-1]}.\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Quantize [Jfk_berlin_address_high.ogg](https://upload.wikimedia.org/wikipedia/commons/3/3a/Jfk_berlin_address_high.ogg) using $4$-bits backward adaptive Jayant quantizer. Reproduce the quantized sequence and provide a subjective comparison with the original sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
