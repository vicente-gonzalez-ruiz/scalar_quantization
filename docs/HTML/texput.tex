% Emacs, this is -*-latex-*-

\title{\href{https://github.com/vicente-gonzalez-ruiz/scalar_quantization}{Scalar (Digital) Quantization}}

\author{Vicente Gonz√°lez Ruiz}

\maketitle
\tableofcontents

\section{Definition}
%{{{

Scalar (Digital)
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{Quantization}~\cite{sayood2017introduction,vetterli2014foundations}
(see Figure~\ref{fig:Q}) is a technique in which each source sample is
quantized independently from the other samples and therefore, a
quantization index ${\mathbf k}_i$ is produced for each input sample
${\mathbf s}_i$~\cite{vruiz__signal_quantization}.

\begin{figure}
  \svgfig{graphics/Q}{3cm}{300pt}
  \caption{Scalar quantization and dequantization of a signal.}
  \label{fig:Q}
\end{figure}

A $K$-levels Scalar Quantizer (SQ) $Q$ performs a partition of the
domain of ${\mathbf s}$ into $K$ cells ${\mathbf C}_k, k = 1, \cdots,
K$ and associates to any ${\mathbf s}_i$ the quantization index ${\mathbf k}_i$ if
${\mathbf s}_i\in {\mathbf C}_k$. In other words,
\begin{equation}
  Q({\mathbf s}_i) = {\mathbf k}_i \Leftrightarrow {\mathbf C}_{k-1} <
  {\mathbf s}_i \le {\mathbf C}_k.
\end{equation}

The inverse quantizer $Q^{-1}$ estimates ${\mathbf s}_i$ knowing
${\mathbf k}_i$ and possibly the PDF (Probability Density Function)
$p_{\mathbf S}({\mathbf s})$, using a reconstruction level ${\mathbf
  r}_k\in ]{\mathbf C}_{k-1}, {\mathbf C}_k]$, generating the output
\begin{equation}
  \tilde{\mathbf s}_i = {\mathbf r}_k.
\end{equation}

The smallest and the highest value of all ${\mathbf C}_k$ are called
the decision boundaries of $Q$. Therefore,

%}}}

\section{Uniform SQ (USQ)}
%{{{

In an USQ, all decision levels are equally spaced by a distance known
as \emph{the Quantization Step Size} (QSS) $\Delta$, satisfiying that
the domain of the input signal is divided into intervals of constant
size
\begin{equation}
  \Delta={\mathbf d}_{i+1}-{\mathbf d}_i={\mathbf r}_{i+1}-{\mathbf r}_i,
\end{equation}
where ${\mathbf d}_i$ is the $i$-th decision level and ${\mathbf r}_i$
is the $i$-th representation level.
  %Therefore,
  %\begin{equation}
  %  Q=\frac{1}{\Delta}
  %\end{equation}
  %and
%We define the number of decision levels as
%\begin{equation}
%  Q=\frac{d_{\text max}-d_{\text min}}{\Delta}.
%  \label{eq:delta_definition}
%\end{equation}

In USQs, the quantization error ${\mathbf e}$ depends on $\Delta$ and
can be modeled as a noise signal that: (1) is uncorrelated to the
input ${\mathbf s}$, (2) is
\href{https://en.wikipedia.org/wiki/White_noise}{white} and therefore,
(3) it follows a uniform distribution.

%However, the quantization
%step
%$\Delta<<\href{https://en.wikipedia.org/wiki/Standard_deviation}{\sigma_x}$~\cite{vetterli1995wavelets}.

%Under the premise that $e$ is uniform, and considering that
%$y_i=(x_{i-1}+x_i)/2$ (something quite reasonable when $x$ can also be
%considered as uniform) the average quantization error is $\frac{Z}{4}$
%($\frac{Z}{2}$ is the meximum and $0$ is the minimum), and for this
%particular case
%\begin{equation}
%  \text{MSE} =
%  \frac{1}{\Delta}\int_{-\Delta/2}^{\Delta/2}e^2de=\frac{\Delta^2}{12}.
%  \label{eq:MSE_uniform_scalar_quantizer}
%\end{equation}

%}}}

\subsection{Mid-rise USQ}
%{{{

In mid-rise quantizers the reconstructed signal $\tilde{\mathbf s}$
never is 0, even if ${\mathbf s}_i=0$ for any $i$. The mapping process
in a mid-rise quantizer can be described as
\begin{equation}
  {\mathbf k}_i = \Big\lfloor \frac{{\mathbf s}_i}{\Delta} \Big\rfloor,
  \label{eq:mid-rise}
\end{equation}
and the inverse mapping by
\begin{equation}
  \tilde{\mathbf s}_i = \Delta\Big({\mathbf k}_i + \frac{1}{2}\Big).
  \label{eq:inverse_mid-rise}
\end{equation}

\begin{figure}
  \svgfig{graphics/midrise}{6cm}{600pt}
  \caption{An uniform mid-rise quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/scalar_quantization/blob/master/graphics/midrise.ipynb}{notebook}). $\Delta=1$
    and $K=13$ (the decision boundaries have been ignored). The
    decision levels (${\mathbf d}$) are $\{\cdots,-3,-2,-1,0,1,2,3,\cdots\}$
    and the representation levels (${\mathbf r}$) are
    $\{\cdots,-2.5,-1.5,-0.5,0.5,1.5,2.5,\cdots\}$.}
  \label{fig:midrise}
\end{figure}
%}}}

\subsection{Mid-tread USQ}
%{{{ 

In mid-tread quantizers the reconstructed signal is $0$ when ${\mathbf
  s}_i=0$. The mapping process in a mid-tread quantizer can be described as
\begin{equation}
  {\mathbf k}_i = \mathrm{round}\Big( \frac{{\mathbf s}_i}{\Delta} \Big),
  \label{eq:midrise}
\end{equation}
and the inverse mapping by
\begin{equation}
  \tilde{\mathbf s}_i = \Delta{\mathbf k}_i.
  \label{eq:inverse_midrise}
\end{equation}

\begin{figure}
  \svgfig{graphics/midtread}{6cm}{600pt}
  \caption{An uniform mid-tread quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/scalar_quantization/blob/master/graphics/midtread.ipynb}{notebook}). $\Delta=1$
    and $K=12$ (the decision boundaries have been ignored). The
    decision levels (${\mathbf d}$) are $\{\cdots,-2.5,-1.5,-0.5,0.5,1.5,2.5,\cdots\}$
    and the representation levels (${\mathbf r}$) are
    $\{\cdots,-2,-1,-0,1,2,\cdots\}$.}
  \label{fig:midtread}
\end{figure}
%}}}

\subsection{Mid-tread USQ with deadzone}
%{{{

In a USQwD (USQ with Deadzone), the quantization step is $2\Delta$ for
${\mathbf s}_i=0$. Deadzone quantizers tends to remove the
\href{https://en.wikipedia.org/wiki/Noise_(electronics)}{electronic
  noise} (that usually has a small amplitude compared to the input
signal ${\mathbf s}$), precisely where the
\href{https://en.wikipedia.org/wiki/Signal-to-noise_ratio}{SNR
  (Signal-to-Noise Ratio)} is the lowest.\footnote{Notice that, by
  definition, dead-zone quantizers should not be considered uniform,
  and that all dead-zone quantizers, by definition, are mid-tread.}

\begin{figure}
  \svgfig{graphics/deadzone}{6cm}{600pt}
  \caption{An uniform deadzone quantizer (see the
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/scalar_quantization/blob/master/graphics/deadzone.ipynb}{notebook}). $\Delta=1$
    and $K=12$ (the decision boundaries have been ignored). The
    decision levels (${\mathbf d}$) are
    $\{\cdots,-3,-2,-1,1,2,3,\cdots\}$ and the representation levels
    (${\mathbf r}$) are $\{\cdots,-2,-1,-0,1,2,\cdots\}$.}
  \label{fig:deadzone}
\end{figure}
%}}}

\section{Non-uniform quantization}
%{{{

Uniform quantizers are efficient (minimize the distortion) only if the
input samples (gray-scale values of the pixels) are uniformly
distributed among the quantization
\href{https://en.wikipedia.org/wiki/Data_binning}{bins}~\cite{vruiz__scalar_quantization}. However,
when the probability of the samples is not ``flat'' (or the number of
bins is small compared to the number of possible input values), we can
use better quantizers. For example, if we know that most of the
samples are small integer values (positive and negative\footnote{Such
  as it happens with audio CD data.}), a quantizer such as
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/gray_SQ/gray_SQ_companded.ipynb}{gray\_SQ\_companded.ipynb}
can be more suitable than an uniform one\footnote{At least, from a
  pure-distortion point of view.}.

If we know that the input signal ${\mathbf s}$ does not follow an
uniform distribution, it is possible to use a variable $\Delta$ to
minimize the quantization error ${\mathbf e}$ in those
samples that are more probable.

%}}}

\subsection{Companded quantization~\cite{sayood2017introduction}}
%{{{

\href{https://en.wikipedia.org/wiki/Companding}{Companding}
(COMpressing + exPANDING) quantization is used when most of the
samples are concentrated arround the $0$ value. For example, most of
the (interesting) audio for humans has a low volume (for this reason,
companded quantizers are used in telephony).

In a companded codec, the original signal is mapped through a
compressor, quantized using an uniform quantized, and re-mapped using
the corresponding expander, resulting in a logarithmic quantization,
centered at $0$. A good example is the
\href{https://en.wikipedia.org/wiki/\%CE\%9C-law_algorithm}{\(\mu\)-law}
codec.

\begin{figure}
  \centering
  \svgfig{graphics/ulaw-compressor}{6cm}{600}
  \svgfig{graphics/ulaw-expander}{6cm}{600}
  \svgfig{graphics/companded}{6cm}{600}
  \caption{Insights of a companded quantizer/dequantizer. See this
    \href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/scalar_quantization/blob/master/graphics/companded_quantization.ipynb}{notebook}.}
  \label{fig:companded_quantizer}
\end{figure}

%}}}

\subsection{PDF-optimized quantization}
%{{{

This non-uniform quantizer is a generalization of the companded
quantizer, where the input samples can follow any distribution. Now,
with the idea of minimizing the distortion (in general, in terms of
the \href{https://en.wikipedia.org/wiki/Mean_squared_error}{MSE}), we
chose ${\mathbf\Delta}_i$ smaller where signal samples appear most
offen.

The most used PDF quantizer is the Max-Lloyd
quantizer~\cite{lloyd1982least}, whose authors developed an iterative
algorithm for determining the decision and representation levels.

\begin{figure}
  \centering
  \svgfig{graphics/cuantif_max-lloyd}{6cm}{600}
  \caption{A Max-Lloyd quantizer.}
  \label{fig:Max-Lloyd}
\end{figure}

Uniform quantizers are efficient (minimize the distortion) only if the
input samples (gray-scale values of the pixels) are uniformly
distributed among the quantization
\href{https://en.wikipedia.org/wiki/Data_binning}{bins}~\cite{vruiz__scalar_quantization}. However,
when the probability of the samples is not ``flat'' (or the number of
bins is small compared to the number of possible input values), we can
use better quantizers. For example, if we know that most of the
samples are small integer values (positive and negative\footnote{Such
as it happens with audio CD data.}), a quantizer such as
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/gray_SQ/gray_SQ_companded.ipynb}{gray\_SQ\_companded.ipynb}
can be more suitable than an uniform one\footnote{At least, from a
  pure-distortion point of view.}.

Notice that the Max-Lloyd quantizer is equivalent to use
K-means~\cite{hartigan1979algorithm} if we known the $K$ parameter
(the number of representation levels). In this case, the centroids
computed by K-means are in the middle of each region. If we don't know
$K$, we must use the Lloyd Algorithm~\cite{hartigan1979algorithm},
which also estimates $K$.

%}}}

\section{Adaptive quantization}
%{{{

Adaptive quantizers modify $\Delta$ dynamically, depending on the local
characteristics of ${\mathbf s}$.

%}}}

\subsection{Forward adaptive quantization}
%{{{

\begin{itemize}
\item
  Used for determining a suitable \(\Delta\) for blocks of samples.
\item ~
  \hypertarget{encoder}{%
  \subsubsection*{Encoder:}\label{forward_encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While samples in \(s\):

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read into \(b\) the next \(B\) samples of \(s\).
    \item
      Determine \(\Delta\), minimizing the quantization error, and
      output \(\Delta\) (or the data necessary for its determination).
    \item
      Quantize \(b\) and output it.
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection*{Decoder:}\label{forward_decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    While data in input:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Read \(\Delta\) (or the data necessary for determining it, and in
      this case, use the same algorithm that the used by the encoder).
    \item
      ``Dequantize'' \(b\) and output it (note that the dequantization
      is only a way of calling the process of reverting the original
      range of the quantized signal).
    \end{enumerate}
  \end{enumerate}
\item
  The selection of \(B\) is a trade-off between the increase in side
  information needed by small block sizes and the loss of fidelity due
  to large block sizes.
\item
  Forward adaptive quantization generates a
  \(B\text{-samples}\times f_s\) delay (buffering), where \(f_s\) is the
  sampling rate of \(s\).
\end{itemize}

%}}}

\section{Backward adaptive quantization}
%{{{

\begin{itemize}
\item
  Only the previously quantized samples are available to use in
  adapting the quantizer.
\item
  Idea: If happens that \(\Delta\) is smaller than it should be, the
  input will fall in the outer levels of the quantizer a high number
  of times. On the other hand, if \(\Delta\) is larger than it should
  be, the samples will fall in the inner levels a high number of
  times.
\item ~
  \hypertarget{encoder}{%
  \subsubsection*{Encoder:}\label{backward_encoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(s\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Quantize the next sample.
    \item
      Observe the output and refine \(\Delta\).
    \end{enumerate}
  \end{enumerate}
\item ~
  \hypertarget{decoder}{%
  \subsubsection*{Decoder:}\label{backward_decoder}}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(\Delta\leftarrow 2\).
  \item
    While \(\hat{s}\) is not exhausted:

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      ``Dequantize'' the next sample.
    \item
      Step 2.B of the encoder.
    \end{enumerate}
  \end{enumerate}
\end{itemize}

%}}}

\subsection{The Jayant quantizer~\cite{jayant1974digital}}
%{{{

\begin{itemize}
\item
  Adaptive quantization with a one word memory (\(\Delta_{(t-1)}\)).
\item
  A Jayant quantider defines the Step 2.B. as: Define a multiplier
  \(M_l\) for each quantization level \(l\), where for the inner levels
  \(M_l<1\) and for the outer levels \(M_l>1\), and compute:

  \[
    \Delta^{[n]} = \Delta^{[n-1]}{M_l}^{[n-1]},
  \]

  where \(\Delta^{[n-1]}\) was the previous quantization step and
  \({M_l}^{[n-1]}\) the level multiplier for the \(n-1\)-th (previous)
  sample. Thus, if the previous (\(n-1\)) quantization used a
  \(\Delta^{[n-1]}\) too small (using outer quantization levels) then
  \(\Delta^{[n]}\) will be larger and viceversa.
\item
  Depending on the multipliers \(M\), the quantizer will converge or
  oscillate. In the first case, the quantizer will be good for small
  variations of \(s\) but bad when a fast adaption to large changes in
  \(s\) is required. In the second one, the quantizer will adapt quickly
  to fast variations of \(s\) but will oscillate when \(s\) changles
  slowly.
\item
  Most Jayant quantizers clip the computation of \(\Delta\) to avoid
  generating a zero output quantizer in those contexts where \(s\) is
  zero or very close to zero, and to improve the adaptation to smaller
  samples after a sequence of bigger ones (avoiding to grow without
  limit):

  \[
  \begin{array}{ll}
    \text{if}~\Delta^{[n]}<\Delta_{\text{min}}~\text{then}~\Delta^{[n]} = \Delta_{\text{min}},\\
    \text{if}~\Delta^{[n]}>\Delta_{\text{max}}~\text{then}~\Delta^{[n]} = \Delta_{\text{max}}.
  \end{array}
  \]
\end{itemize}

%}}}

\subsection{Adapting with a scale factor}
%{{{

\begin{itemize}
\item
  A Jayant quantized adapts the quantization step to the dynamic range
  of the signa using a set of multipiers. A similar effect can be
  provided by dividing the input signal by a scale factor defined
  iteratively as:

  \begin{equation}
    \alpha^{[n]} = \alpha^{[n-1]}M_l^{[n-1]}.
  \end{equation}
\end{itemize}

%}}}

\section{RD performance}

Normaly, RD curves are convex~\cite{vruiz__information_theory} (this
can be seen in the notebooks
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/scalar_quantization/midtread.ipynb}{Midtread Quantization},
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/scalar_quantization/midrise.ipynb}{Midrise Quantization},
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/scalar_quantization/deadzone.ipynb}{Deadzone Quantization},
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/scalar_quantization/companded.ipynb}{Companded Quantization},
and
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/scalar_quantization/LloydMax.ipynb}{LloydMax Quantization}). This means that:
\begin{enumerate}
\item At low bit-rates the distortion decreases faster than at high
  bit-rates.
\item If we have a \emph{scalable} code-stream (we can decide how the
  code-stream will be decompressed), we should be aware that some
  parts of the code-stream can minimize faster the RD curve than
  others.
\end{enumerate}

As it can be also seen in the notebook
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/scalar_quantization/compare_quantizers.ipynb}{quantizers\_comparison},
that the performance of the quantizers is not the same: usually
midrise and midtread, performs better than deadzone at intermediate
bit-rates, but deadzone is the best a low bit-rates (excluding
Lloyd-Max). Deadzone has also another advantage over midread and
midtread: when $\Delta$ is a power of 2 (which corresponds to a
bit-plane encoding), the obtained RD point is near optimal in the RD
space. Finally, the Lloyd-Max Quantizer reaches the highest
performance because it is adaptive.

\section{Perceptual quantization}
%{{{

\subsection{In audio}

An important consideration is the relative perfectual importance of
the input samples. This leads to a weighting of the MSE at the
output. The weighting function can be derived through experiments to
determine the ``level of just noticeable noise''. For example, in
subband coding, as expected, high frequecy subbands tolerate more
noise because the HAS (Human Auditory System) becomes less sensitive
at them.

\subsection{In images}

Normally, \href{https://en.wikipedia.org/wiki/Visual_system}{humans}
hardly distinguish more than 64 different
\href{https://en.wikipedia.org/wiki/Color}{colors}.

%}}}

\section{Quantization error}
The quantization error can be modeled as a random signal $\mathbf{e}$
that is added to the original one $\mathbf{s}$, and the energy of this
error signal $\langle \mathbf{e},\mathbf{e}\rangle$ depends on
the QSS and the values of $\mathbf{s}$.

\section{RD optimized quantizer design}
So far, we have designed the image compressor in two steps: (1)
quantize the pixels ... trying to minimize the distortion, and (2) entropy
encode (compress) the quantization indexes. However, if we assume that
the entropy encoding performance is proportional to some
rate-prediction metric, such as the entropy, it is possible to design
the quantizer minizing the two RD variables (rate and distortion) at
the same time~\cite{sayood2017introduction}.

However, there is not (yet) any known reason to think that this
approach will be generate better RD curves that the one in which only
the distortion is both parameters (R and D) are minimized
independently. Besides, the joint optimization of R and D makes the
quantizer dependent of the entropy codec, that reduces the flexility
of the encoding system.

\section{Resources}
\bibliography{quantization,DWT,data-compression,pattern_recognition}
